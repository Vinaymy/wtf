{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "high_freq_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FcCAH06vXc8W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This file contains the baseline model and the final model I built on F.G.'s handwritten journal image cropped tiles. The main file for the project. Also, in this file I select the most frequent 7 classes and the model is built to predict these classes. I used the top 7 most frequent since they have at least 300 labeled examples.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "y4D5eBqpW1Jb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First I list all the imports used this file."
      ]
    },
    {
      "metadata": {
        "id": "Ue_c4pryXcE9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import model_from_json\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imread\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from random import seed, sample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from collections import Counter\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19VPbQJ9Xw2E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ld.tar contains the tarred labeled_data directory from https://github.com/Vinaymy/wtf/tree/master/data/labeled_data. The next line untars this file and collects data in the local directory of execution."
      ]
    },
    {
      "metadata": {
        "id": "7-yrNPsEncs-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xvf ld.tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pr8tpnSoYWYZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next I load up the cropped labeled images into memory and map them to class labels. I map upper and lower case English alphabet to the same class. Along with 10 digits this gives me 36 classes. The labels for the classes are clear from the mapping dictionary in the code below.\n",
        "\n",
        "Also, I set a frequency lower bound of 300 for selecting a character class for the prototype CNN, so that I have enough data to feed the CNN. 7 character classes pass this filter - A/a, D/d, E/e, N/n, O/o, R/r and S/s.\n",
        "\n",
        "Also, I limit the number of examples used for each of the classes to exactly 300 even though some classes (e.g. E/e) have more examples in the labeled set. This is so I have a balanced distribution of all classes. I empirically found that imbalanced classes cause the model to overfit and do terribly on the test set."
      ]
    },
    {
      "metadata": {
        "id": "NdiOZ8adnqhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cbd9b629-699f-469b-83af-9e729b741308"
      },
      "cell_type": "code",
      "source": [
        "rootDir = './labeled_data'\n",
        "n_images = 10000\n",
        "images = np.zeros((n_images,) + (20, 20))\n",
        "minimum_number_of_examples = 300 # I came up with this after inspecting the distribution of frequencies of the labeled classes. \n",
        "# I decided that images with fewer than examples is too few to try predict.\n",
        "\n",
        "labels = []\n",
        "lab = {\n",
        "    '0': 1, '1': 2, '2': 3, '3':4, '4':5, '5':6, '6':7, '7':8, '8':9, '9':10,\n",
        "    'A': 11, 'B': 12, 'C': 13, 'D': 14, 'E': 15, 'F': 16, 'G': 17, 'H': 18, 'I': 19, 'J': 20, 'K': 21, 'L': 22, 'M': 23, \n",
        "    'N': 24, 'O': 25, 'P': 26, 'Q': 27, 'R': 28, 'S': 29, 'T': 30, 'U': 31, 'V': 32, 'W': 33, 'X': 34, 'Y': 35, 'Z': 36,\n",
        "    'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, \n",
        "    'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36\n",
        "}\n",
        "j = 0\n",
        "ct = {}\n",
        "for dirName, subdirList, fileList in os.walk(rootDir):\n",
        "    i = 0\n",
        "    for fname in fileList:\n",
        "        if 'png' not in fname or fname[:3] !='IMG':\n",
        "            continue\n",
        "        full_path = os.path.join(dirName, fname)\n",
        "        \n",
        "        if dirName[-1] in ['E', 'e', 'A', 'a', 'T', 't', 'S', 's', 'D', 'd', 'N', 'n', 'O', 'o'] \\\n",
        "        and dirName[-2] == '/' and ct.get(dirName[-1].lower(), 0) < minimum_number_of_examples:\n",
        "            ct[dirName[-1].lower()] = ct.get(dirName[-1].lower(), 0) + 1\n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "        # Resize image  to 20 X 20\n",
        "        images[j] = resize(imread(full_path, as_grey=True), (20, 20)) \n",
        "        labels.append(lab[dirName[-1]])\n",
        "        j += 1\n",
        "        i += 1\n",
        "images = images[:len(labels)]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "H6H6juvzYfuf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the next part I inspect the number of examples selected for each label and ensure that it is 300 exactly.\n"
      ]
    },
    {
      "metadata": {
        "id": "_oyuOcnMPUva",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "count = Counter(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G21jGSKpQPL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "527be5d4-893e-42d1-e95a-0e5af8b2f75a"
      },
      "cell_type": "code",
      "source": [
        "count.most_common() # 7 labels of and balanced since I picked 300 of each"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(29, 300), (15, 300), (14, 300), (30, 300), (24, 300), (25, 300), (11, 300)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "TFDL7-phYdfj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us examine the labels that passed the filter of at least 300 examples."
      ]
    },
    {
      "metadata": {
        "id": "V28ky1hIyoOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3152d9c4-172d-405c-f17d-c08756a0b698"
      },
      "cell_type": "code",
      "source": [
        "np.unique(labels) "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11, 14, 15, 24, 25, 29, 30])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "uSLFJ__dYsCR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above number labels correspond to [A/a, D/d. E/e, N/n, O/o, R/r, S/s]"
      ]
    },
    {
      "metadata": {
        "id": "XGGhuwyuZemS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Perform 80% - 20% split of examples to get a traiing-set/testing set split. Then we'll have 1680 training set image tiles and 420 test set image tiles."
      ]
    },
    {
      "metadata": {
        "id": "HZQrOO_l5Hqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "84b39ddf-abd8-4de8-e36c-ca0a161fa59d"
      },
      "cell_type": "code",
      "source": [
        "k = sample(range(len(images)), len(images))\n",
        "im_shuf = images[k]\n",
        "labels_shuf = np.array(labels)[k]\n",
        "x_train, x_test, y_train, y_test = train_test_split(im_shuf, labels_shuf, random_state=2, train_size=0.8)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "C9uVhR-qZoRw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Examine the size of the training set and confirm its 1680 examples."
      ]
    },
    {
      "metadata": {
        "id": "8iufD5956Hkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c9aefd2-8bd2-4653-881a-81eb70a624f7"
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1680,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "igy_5RLsZvFs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Build a baseline model and record its accuracy on the test set for reference. I build a linear kernel SVM with grid search on hyperparameter C. I get an accuracy of 0.47 on the training set and 0.23 on the test set for the baseline model. "
      ]
    },
    {
      "metadata": {
        "id": "GEOyGfYS3DTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1802
        },
        "outputId": "24943d98-0ef9-49db-fbfb-3f5ef0015bf7"
      },
      "cell_type": "code",
      "source": [
        "base_model = LinearSVC()\n",
        "param_grid = {'C':  list(np.arange(0.1,1.5,0.1))}\n",
        "\n",
        "gs = GridSearchCV(base_model, param_grid, n_jobs=-1, cv=3, verbose=4)\n",
        "x_train_resh = x_train.reshape((x_train.shape[0], -1))\n",
        "x_test_resh = x_test.reshape((x_test.shape[0], -1))\n",
        "gs.fit(x_train_resh, y_train)\n",
        "pprint(sorted(gs.grid_scores_, key=lambda x: -x.mean_validation_score))\n",
        "\n",
        "y_pred = gs.predict(x_test_resh)\n",
        "print ('Test set shape: ', x_test_resh.shape)\n",
        "print ('Target shape: ', y_test.shape)\n",
        "print ('Accuracy on train set: ', accuracy_score(y_train, gs.predict(x_train_resh)))\n",
        "print ('Accuracy on test set: ', accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n",
            "[CV] C=0.1 ...........................................................\n",
            "[CV] C=0.1 ...........................................................\n",
            "[CV] .................................. C=0.1, score=0.225577 -   6.3s\n",
            "[CV] C=0.1 ...........................................................\n",
            "[CV] .................................. C=0.1, score=0.210714 -   7.0s\n",
            "[CV] C=0.2 ...........................................................\n",
            "[CV] .................................. C=0.1, score=0.222621 -   8.0s\n",
            "[CV] C=0.2 ...........................................................\n",
            "[CV] .................................. C=0.2, score=0.211368 -  10.2s\n",
            "[CV] C=0.2 ...........................................................\n",
            "[CV] .................................. C=0.2, score=0.207143 -  10.2s\n",
            "[CV] C=0.30000000000000004 ...........................................\n",
            "[CV] .................................. C=0.2, score=0.211849 -  10.4s\n",
            "[CV] C=0.30000000000000004 ...........................................\n",
            "[CV] .................. C=0.30000000000000004, score=0.213144 -  10.0s\n",
            "[CV] C=0.30000000000000004 ...........................................\n",
            "[CV] .................. C=0.30000000000000004, score=0.200000 -   9.9s\n",
            "[CV] C=0.4 ...........................................................\n",
            "[CV] .................. C=0.30000000000000004, score=0.206463 -   9.9s\n",
            "[CV] C=0.4 ...........................................................\n",
            "[CV] .................................. C=0.4, score=0.211368 -  10.1s\n",
            "[CV] C=0.4 ...........................................................\n",
            "[CV] .................................. C=0.4, score=0.201786 -  10.1s\n",
            "[CV] C=0.5 ...........................................................\n",
            "[CV] .................................. C=0.4, score=0.202873 -  10.1s\n",
            "[CV] C=0.5 ...........................................................\n",
            "[CV] .................................. C=0.5, score=0.204263 -  10.2s\n",
            "[CV] C=0.5 ...........................................................\n",
            "[CV] .................................. C=0.5, score=0.198214 -  10.2s\n",
            "[CV] C=0.6 ...........................................................\n",
            "[CV] .................................. C=0.5, score=0.201077 -   9.8s\n",
            "[CV] C=0.6 ...........................................................\n",
            "[CV] .................................. C=0.6, score=0.204263 -   9.7s\n",
            "[CV] C=0.6 ...........................................................\n",
            "[CV] .................................. C=0.6, score=0.192857 -   9.7s\n",
            "[CV] C=0.7000000000000001 ............................................\n",
            "[CV] .................................. C=0.6, score=0.202873 -   9.7s\n",
            "[CV] C=0.7000000000000001 ............................................\n",
            "[CV] ................... C=0.7000000000000001, score=0.204263 -   9.7s\n",
            "[CV] C=0.7000000000000001 ............................................\n",
            "[CV] ................... C=0.7000000000000001, score=0.189286 -   9.9s\n",
            "[CV] C=0.8 ...........................................................\n",
            "[CV] ................... C=0.7000000000000001, score=0.195691 -   9.7s\n",
            "[CV] C=0.8 ...........................................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.7min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .................................. C=0.8, score=0.206039 -   9.9s\n",
            "[CV] C=0.8 ...........................................................\n",
            "[CV] .................................. C=0.8, score=0.196429 -   9.7s\n",
            "[CV] C=0.9 ...........................................................\n",
            "[CV] .................................. C=0.8, score=0.197487 -   9.8s\n",
            "[CV] C=0.9 ...........................................................\n",
            "[CV] .................................. C=0.9, score=0.204263 -   9.9s\n",
            "[CV] C=0.9 ...........................................................\n",
            "[CV] .................................. C=0.9, score=0.192857 -  10.0s\n",
            "[CV] C=1.0 ...........................................................\n",
            "[CV] .................................. C=0.9, score=0.201077 -   9.9s\n",
            "[CV] C=1.0 ...........................................................\n",
            "[CV] .................................. C=1.0, score=0.211368 -   9.8s\n",
            "[CV] C=1.0 ...........................................................\n",
            "[CV] .................................. C=1.0, score=0.200000 -   9.7s\n",
            "[CV] C=1.1 ...........................................................\n",
            "[CV] .................................. C=1.0, score=0.199282 -   9.8s\n",
            "[CV] C=1.1 ...........................................................\n",
            "[CV] .................................. C=1.1, score=0.204263 -   9.8s\n",
            "[CV] C=1.1 ...........................................................\n",
            "[CV] .................................. C=1.1, score=0.200000 -   9.9s\n",
            "[CV] C=1.2000000000000002 ............................................\n",
            "[CV] .................................. C=1.1, score=0.208259 -   9.5s\n",
            "[CV] C=1.2000000000000002 ............................................\n",
            "[CV] ................... C=1.2000000000000002, score=0.202487 -   9.6s\n",
            "[CV] C=1.2000000000000002 ............................................\n",
            "[CV] ................... C=1.2000000000000002, score=0.189286 -   9.3s\n",
            "[CV] C=1.3000000000000003 ............................................\n",
            "[CV] ................... C=1.2000000000000002, score=0.188510 -   9.5s\n",
            "[CV] C=1.3000000000000003 ............................................\n",
            "[CV] ................... C=1.3000000000000003, score=0.193606 -   9.6s\n",
            "[CV] C=1.3000000000000003 ............................................\n",
            "[CV] ................... C=1.3000000000000003, score=0.185714 -   9.9s\n",
            "[CV] C=1.4000000000000001 ............................................\n",
            "[CV] ................... C=1.3000000000000003, score=0.201077 -   9.4s\n",
            "[CV] C=1.4000000000000001 ............................................\n",
            "[CV] ................... C=1.4000000000000001, score=0.202487 -   9.6s\n",
            "[CV] C=1.4000000000000001 ............................................\n",
            "[CV] ................... C=1.4000000000000001, score=0.205357 -   9.4s\n",
            "[CV] ................... C=1.4000000000000001, score=0.170557 -   7.9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:  3.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[mean: 0.21964, std: 0.00642, params: {'C': 0.1},\n",
            " mean: 0.21012, std: 0.00211, params: {'C': 0.2},\n",
            " mean: 0.20655, std: 0.00537, params: {'C': 0.30000000000000004},\n",
            " mean: 0.20536, std: 0.00428, params: {'C': 0.4},\n",
            " mean: 0.20417, std: 0.00337, params: {'C': 1.1},\n",
            " mean: 0.20357, std: 0.00554, params: {'C': 1.0},\n",
            " mean: 0.20119, std: 0.00247, params: {'C': 0.5},\n",
            " mean: 0.20000, std: 0.00508, params: {'C': 0.6},\n",
            " mean: 0.20000, std: 0.00430, params: {'C': 0.8},\n",
            " mean: 0.19940, std: 0.00481, params: {'C': 0.9},\n",
            " mean: 0.19643, std: 0.00614, params: {'C': 0.7000000000000001},\n",
            " mean: 0.19345, std: 0.00641, params: {'C': 1.2000000000000002},\n",
            " mean: 0.19345, std: 0.00627, params: {'C': 1.3000000000000003},\n",
            " mean: 0.19286, std: 0.01577, params: {'C': 1.4000000000000001}]\n",
            "Test set shape:  (420, 400)\n",
            "Target shape:  (420,)\n",
            "Accuracy on train set:  0.4720238095238095\n",
            "Accuracy on test set:  0.23333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bd-2Eq3LaCuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next I build the target model CNN. I use a batch size of 128 for training, this is a pretty good batch size since it's reasonable large compared to the total data size, and also runs pretty efficiently. Also, I one hot encode the target labels."
      ]
    },
    {
      "metadata": {
        "id": "byJKrFCrlak_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = len(uniques)+1\n",
        "batch_size = 128\n",
        "\n",
        "X_train = x_train.reshape(x_train.shape[0], 20, 20 , 1).astype('float32')\n",
        "X_test = x_test.reshape(x_test.shape[0], 20, 20 , 1).astype('float32')\n",
        "\n",
        "uniques, ids_tr = np.unique(y_train, return_inverse=True)\n",
        "uniques, ids_te = np.unique(y_test, return_inverse=True)\n",
        "\n",
        "Y_train = to_categorical(ids_tr, num_classes)\n",
        "Y_test = to_categorical(ids_te, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pwmIIHUMag8w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Examine a sample from the one hot encoded test set. The below is an example of an e. Index 0 in the one-hot encoding is not used."
      ]
    },
    {
      "metadata": {
        "id": "Ny2ZZ6AnJwYA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b16234e-bdfc-4cc2-e718-e709ce1b5c05"
      },
      "cell_type": "code",
      "source": [
        "Y_test[0]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "BKQjZ-EYaLMT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Specify the model architecture and train it and test it. I get a training accuracy of 0.941 and a test accuracy of 0.807. \n",
        "\n",
        "I used 70 epochs and batch size of 128 arrived through trial and error. \n",
        "\n",
        "I add a wrapper method for creating the model, since in the next steps I perform a 5-fold cross validation analysis and I want to modularize the creation of the model architecture so that I can reuse this code."
      ]
    },
    {
      "metadata": {
        "id": "4pVBfBlp5FDQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_network(num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(20, 20, 1)))\n",
        "\n",
        "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "\n",
        "    model.add(Conv2D(128, (5, 5), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OuMwqegqcUJ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next call the above method and actually create a model and train it on the training set and test it on the test set."
      ]
    },
    {
      "metadata": {
        "id": "qSy9MduRTbKK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = create_network(num_classes)\n",
        "model.summary()\n",
        "model.fit(X_train, Y_train, batch_size=batch_size,\n",
        "                        nb_epoch=70,\n",
        "                        verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FogihFfB7dCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb4e7f43-c723-424f-bf11-08efb39b39e9"
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(X_test, Y_test, verbose = 10 )\n",
        "print ( scores )\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.2459780988239106, 0.6928571428571428]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S0ChLoHWch5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next I generate a multi-class confusion matrix. "
      ]
    },
    {
      "metadata": {
        "id": "-JK2Jk-uAhpo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_predict_non_category = [ np.unique(labels)[np.argmax(t)] for t in y_predict ]\n",
        "\n",
        "conf_mat = confusion_matrix(y_test, y_predict_non_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gSu5jC6GH5zf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "26291926-c03c-4c51-a5a4-eaaf1c5e159b"
      },
      "cell_type": "code",
      "source": [
        "conf_mat"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[54,  0,  0,  2,  2,  0,  0],\n",
              "       [ 8, 56,  0,  0,  1,  1,  1],\n",
              "       [ 1,  0, 44,  1,  1,  2,  0],\n",
              "       [ 0,  0,  0, 60,  0,  0,  0],\n",
              "       [ 1,  0,  0,  5, 53,  1,  0],\n",
              "       [ 3,  0,  0,  2,  1, 65,  1],\n",
              "       [ 1,  0,  0,  0,  0,  4, 49]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "QVnRes-Oc3Gs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let us do some robustness analysis and check for overfitting through a 5 fold cross validation across the full dataset, to understand the training process and performance better. I print out the training accuracy and loss at every epoch and also testing loss and accuracy at the end training for each fold. \n",
        "\n",
        "This analusis shows that the model is training quite well and showing a healthy change in loss and accuracy over 70 epochs consistently across the folds. Hoeever, I do see some signs of overfitting, since the accuracy on the holdout set is typically in the 70 - 80 % range. This indicates reasonably clearly that the very next step should be to get more labeled data. However due to class imbalance I could be selective about what type of labeled data to collect. I have more details on this in my report. \n",
        "\n",
        "Another strategy I may need to try after this project is to experiment with additional dropout."
      ]
    },
    {
      "metadata": {
        "id": "L71ogaj0TpLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12036
        },
        "outputId": "919d4b27-72d4-4d55-c98a-ef5ec94a0967"
      },
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=5)\n",
        "\n",
        "X_valid = im_shuf.reshape(im_shuf.shape[0], 20, 20 , 1).astype('float32')\n",
        "\n",
        "uniques, ids_valid = np.unique(labels_shuf, return_inverse=True)\n",
        "\n",
        "Y_valid = to_categorical(ids_valid, num_classes)\n",
        "\n",
        "for train_indices, test_indices in kf.split(X_valid):\n",
        "    model = create_network(num_classes = len(uniques)+1)\n",
        "    model.fit(X_valid[train_indices], Y_valid[train_indices], batch_size=batch_size,\n",
        "                        nb_epoch=70,\n",
        "                        verbose=1)\n",
        "    scores = model.evaluate(X_valid[test_indices], Y_valid[test_indices], verbose = 10 )\n",
        "    print(scores)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "1680/1680 [==============================] - 7s 4ms/step - loss: 2.0431 - acc: 0.1381\n",
            "Epoch 2/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 2.0057 - acc: 0.1381\n",
            "Epoch 3/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 2.0037 - acc: 0.1464\n",
            "Epoch 4/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9839 - acc: 0.1357\n",
            "Epoch 5/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9723 - acc: 0.1494\n",
            "Epoch 6/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9845 - acc: 0.1369\n",
            "Epoch 7/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9684 - acc: 0.1601\n",
            "Epoch 8/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9740 - acc: 0.1435\n",
            "Epoch 9/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9698 - acc: 0.1464\n",
            "Epoch 10/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9670 - acc: 0.1607\n",
            "Epoch 11/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9511 - acc: 0.1780\n",
            "Epoch 12/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9425 - acc: 0.1804\n",
            "Epoch 13/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9228 - acc: 0.2042\n",
            "Epoch 14/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9316 - acc: 0.1964\n",
            "Epoch 15/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.8760 - acc: 0.2381\n",
            "Epoch 16/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.8915 - acc: 0.2565\n",
            "Epoch 17/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.8479 - acc: 0.2720\n",
            "Epoch 18/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.7696 - acc: 0.3089\n",
            "Epoch 19/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.8119 - acc: 0.3012\n",
            "Epoch 20/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.7398 - acc: 0.3167\n",
            "Epoch 21/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.5966 - acc: 0.3994\n",
            "Epoch 22/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.5033 - acc: 0.4351\n",
            "Epoch 23/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.4345 - acc: 0.4631\n",
            "Epoch 24/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.4709 - acc: 0.4577\n",
            "Epoch 25/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.3066 - acc: 0.5250\n",
            "Epoch 26/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.2340 - acc: 0.5494\n",
            "Epoch 27/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.1121 - acc: 0.6083\n",
            "Epoch 28/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.1922 - acc: 0.5744\n",
            "Epoch 29/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.9551 - acc: 0.6720\n",
            "Epoch 30/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.9954 - acc: 0.6393\n",
            "Epoch 31/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.8680 - acc: 0.7054\n",
            "Epoch 32/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.8300 - acc: 0.7131\n",
            "Epoch 33/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.8186 - acc: 0.7006\n",
            "Epoch 34/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.9888 - acc: 0.6631\n",
            "Epoch 35/70\n",
            "1680/1680 [==============================] - 7s 4ms/step - loss: 0.6319 - acc: 0.7732\n",
            "Epoch 36/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.6722 - acc: 0.7738\n",
            "Epoch 37/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.5855 - acc: 0.7929\n",
            "Epoch 38/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.6369 - acc: 0.7750\n",
            "Epoch 39/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.5534 - acc: 0.8155\n",
            "Epoch 40/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.5154 - acc: 0.8238\n",
            "Epoch 41/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.5495 - acc: 0.8089\n",
            "Epoch 42/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4796 - acc: 0.8363\n",
            "Epoch 43/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4304 - acc: 0.8583\n",
            "Epoch 44/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4002 - acc: 0.8583\n",
            "Epoch 45/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4251 - acc: 0.8429\n",
            "Epoch 46/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4053 - acc: 0.8560\n",
            "Epoch 47/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3714 - acc: 0.8756\n",
            "Epoch 48/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3887 - acc: 0.8619\n",
            "Epoch 49/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4165 - acc: 0.8786\n",
            "Epoch 50/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3361 - acc: 0.8768\n",
            "Epoch 51/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3430 - acc: 0.8857\n",
            "Epoch 52/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2965 - acc: 0.8982\n",
            "Epoch 53/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3290 - acc: 0.8917\n",
            "Epoch 54/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2620 - acc: 0.9208\n",
            "Epoch 55/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2496 - acc: 0.9095\n",
            "Epoch 56/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2318 - acc: 0.9220\n",
            "Epoch 57/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2648 - acc: 0.9065\n",
            "Epoch 58/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2279 - acc: 0.9202\n",
            "Epoch 59/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2451 - acc: 0.9149\n",
            "Epoch 60/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.1852 - acc: 0.9357\n",
            "Epoch 61/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3037 - acc: 0.9030\n",
            "Epoch 62/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.1787 - acc: 0.9446\n",
            "Epoch 63/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.1697 - acc: 0.9429\n",
            "Epoch 64/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2070 - acc: 0.9298\n",
            "Epoch 65/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.1857 - acc: 0.9393\n",
            "Epoch 66/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.1677 - acc: 0.9417\n",
            "Epoch 67/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2048 - acc: 0.9238\n",
            "Epoch 68/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.1520 - acc: 0.9452\n",
            "Epoch 69/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2015 - acc: 0.9351\n",
            "Epoch 70/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.1300 - acc: 0.9595\n",
            "[1.0150810559590657, 0.7476190476190476]\n",
            "Epoch 1/70\n",
            "1680/1680 [==============================] - 7s 4ms/step - loss: 2.0467 - acc: 0.1411\n",
            "Epoch 2/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 2.0027 - acc: 0.1357\n",
            "Epoch 3/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9853 - acc: 0.1393\n",
            "Epoch 4/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9850 - acc: 0.1440\n",
            "Epoch 5/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9753 - acc: 0.1476\n",
            "Epoch 6/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9723 - acc: 0.1464\n",
            "Epoch 7/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9657 - acc: 0.1446\n",
            "Epoch 8/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9619 - acc: 0.1440\n",
            "Epoch 9/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9631 - acc: 0.1470\n",
            "Epoch 10/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9668 - acc: 0.1399\n",
            "Epoch 11/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9644 - acc: 0.1601\n",
            "Epoch 12/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9478 - acc: 0.1685\n",
            "Epoch 13/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9470 - acc: 0.1649\n",
            "Epoch 14/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9416 - acc: 0.1762\n",
            "Epoch 15/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9618 - acc: 0.1643\n",
            "Epoch 16/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9278 - acc: 0.1869\n",
            "Epoch 17/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.9100 - acc: 0.2173\n",
            "Epoch 18/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9005 - acc: 0.2250\n",
            "Epoch 19/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9079 - acc: 0.2244\n",
            "Epoch 20/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.8418 - acc: 0.2631\n",
            "Epoch 21/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8729 - acc: 0.2494\n",
            "Epoch 22/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8074 - acc: 0.2798\n",
            "Epoch 23/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.7820 - acc: 0.3030\n",
            "Epoch 24/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.7153 - acc: 0.3333\n",
            "Epoch 25/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.6790 - acc: 0.3649\n",
            "Epoch 26/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.6150 - acc: 0.3905\n",
            "Epoch 27/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.6701 - acc: 0.3768\n",
            "Epoch 28/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.4943 - acc: 0.4357\n",
            "Epoch 29/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.3991 - acc: 0.4994\n",
            "Epoch 30/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.4599 - acc: 0.4786\n",
            "Epoch 31/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.3033 - acc: 0.5196\n",
            "Epoch 32/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.2779 - acc: 0.5435\n",
            "Epoch 33/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.1495 - acc: 0.5839\n",
            "Epoch 34/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.0870 - acc: 0.6024\n",
            "Epoch 35/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.0735 - acc: 0.6214\n",
            "Epoch 36/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0643 - acc: 0.6107\n",
            "Epoch 37/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.9229 - acc: 0.6768\n",
            "Epoch 38/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.9464 - acc: 0.6655\n",
            "Epoch 39/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7964 - acc: 0.7155\n",
            "Epoch 40/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.8027 - acc: 0.7113\n",
            "Epoch 41/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7254 - acc: 0.7446\n",
            "Epoch 42/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7167 - acc: 0.7494\n",
            "Epoch 43/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6975 - acc: 0.7583\n",
            "Epoch 44/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5945 - acc: 0.7881\n",
            "Epoch 45/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.6057 - acc: 0.7929\n",
            "Epoch 46/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.6293 - acc: 0.7905\n",
            "Epoch 47/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.5232 - acc: 0.8131\n",
            "Epoch 48/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4704 - acc: 0.8369\n",
            "Epoch 49/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4982 - acc: 0.8196\n",
            "Epoch 50/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5165 - acc: 0.8268\n",
            "Epoch 51/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4223 - acc: 0.8464\n",
            "Epoch 52/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4506 - acc: 0.8464\n",
            "Epoch 53/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4116 - acc: 0.8643\n",
            "Epoch 54/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4566 - acc: 0.8446\n",
            "Epoch 55/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3501 - acc: 0.8851\n",
            "Epoch 56/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3402 - acc: 0.8827\n",
            "Epoch 57/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4432 - acc: 0.8548\n",
            "Epoch 58/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3623 - acc: 0.8762\n",
            "Epoch 59/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2992 - acc: 0.8982\n",
            "Epoch 60/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2827 - acc: 0.9036\n",
            "Epoch 61/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3047 - acc: 0.8982\n",
            "Epoch 62/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2966 - acc: 0.8911\n",
            "Epoch 63/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2713 - acc: 0.9071\n",
            "Epoch 64/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2387 - acc: 0.9214\n",
            "Epoch 65/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2438 - acc: 0.9185\n",
            "Epoch 66/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2561 - acc: 0.9167\n",
            "Epoch 67/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2604 - acc: 0.9196\n",
            "Epoch 68/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.1999 - acc: 0.9315\n",
            "Epoch 69/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2235 - acc: 0.9208\n",
            "Epoch 70/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.2042 - acc: 0.9280\n",
            "[1.5744809014456613, 0.6047619047619047]\n",
            "Epoch 1/70\n",
            "1680/1680 [==============================] - 7s 4ms/step - loss: 2.0408 - acc: 0.1375\n",
            "Epoch 2/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 2.0098 - acc: 0.1524\n",
            "Epoch 3/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9918 - acc: 0.1476\n",
            "Epoch 4/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9859 - acc: 0.1435\n",
            "Epoch 5/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9952 - acc: 0.1399\n",
            "Epoch 6/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9849 - acc: 0.1315\n",
            "Epoch 7/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9776 - acc: 0.1524\n",
            "Epoch 8/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9778 - acc: 0.1405\n",
            "Epoch 9/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9734 - acc: 0.1536\n",
            "Epoch 10/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9668 - acc: 0.1512\n",
            "Epoch 11/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9525 - acc: 0.1798\n",
            "Epoch 12/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9299 - acc: 0.2000\n",
            "Epoch 13/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9302 - acc: 0.2202\n",
            "Epoch 14/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9043 - acc: 0.2256\n",
            "Epoch 15/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8560 - acc: 0.2405\n",
            "Epoch 16/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8272 - acc: 0.2744\n",
            "Epoch 17/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.7916 - acc: 0.2815\n",
            "Epoch 18/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8476 - acc: 0.2893\n",
            "Epoch 19/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.7794 - acc: 0.3077\n",
            "Epoch 20/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.6071 - acc: 0.3875\n",
            "Epoch 21/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.5754 - acc: 0.4131\n",
            "Epoch 22/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.5168 - acc: 0.4274\n",
            "Epoch 23/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.5202 - acc: 0.4125\n",
            "Epoch 24/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.4421 - acc: 0.4637\n",
            "Epoch 25/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.2585 - acc: 0.5464\n",
            "Epoch 26/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.2426 - acc: 0.5595\n",
            "Epoch 27/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.1787 - acc: 0.5738\n",
            "Epoch 28/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.1832 - acc: 0.5726\n",
            "Epoch 29/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0931 - acc: 0.6012\n",
            "Epoch 30/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0760 - acc: 0.6125\n",
            "Epoch 31/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0199 - acc: 0.6208\n",
            "Epoch 32/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.9084 - acc: 0.6768\n",
            "Epoch 33/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.8479 - acc: 0.6988\n",
            "Epoch 34/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7941 - acc: 0.7137\n",
            "Epoch 35/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7735 - acc: 0.7333\n",
            "Epoch 36/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7661 - acc: 0.7286\n",
            "Epoch 37/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6996 - acc: 0.7494\n",
            "Epoch 38/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6709 - acc: 0.7542\n",
            "Epoch 39/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6267 - acc: 0.7792\n",
            "Epoch 40/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6612 - acc: 0.7685\n",
            "Epoch 41/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5916 - acc: 0.7863\n",
            "Epoch 42/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5964 - acc: 0.7893\n",
            "Epoch 43/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5600 - acc: 0.8113\n",
            "Epoch 44/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5181 - acc: 0.8167\n",
            "Epoch 45/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4790 - acc: 0.8256\n",
            "Epoch 46/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5436 - acc: 0.8089\n",
            "Epoch 47/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4662 - acc: 0.8274\n",
            "Epoch 48/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4051 - acc: 0.8690\n",
            "Epoch 49/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5534 - acc: 0.8179\n",
            "Epoch 50/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3764 - acc: 0.8768\n",
            "Epoch 51/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.4305 - acc: 0.8458\n",
            "Epoch 52/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 0.3625 - acc: 0.8738\n",
            "Epoch 53/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3596 - acc: 0.8690\n",
            "Epoch 54/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3635 - acc: 0.8810\n",
            "Epoch 55/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3567 - acc: 0.8792\n",
            "Epoch 56/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3658 - acc: 0.8774\n",
            "Epoch 57/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3011 - acc: 0.8923\n",
            "Epoch 58/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3089 - acc: 0.8923\n",
            "Epoch 59/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2587 - acc: 0.9155\n",
            "Epoch 60/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2509 - acc: 0.9101\n",
            "Epoch 61/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2614 - acc: 0.9137\n",
            "Epoch 62/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2534 - acc: 0.9095\n",
            "Epoch 63/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2274 - acc: 0.9244\n",
            "Epoch 64/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2303 - acc: 0.9268\n",
            "Epoch 65/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2425 - acc: 0.9101\n",
            "Epoch 66/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2916 - acc: 0.8911\n",
            "Epoch 67/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2245 - acc: 0.9304\n",
            "Epoch 68/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.1993 - acc: 0.9220\n",
            "Epoch 69/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2219 - acc: 0.9286\n",
            "Epoch 70/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.1914 - acc: 0.9357\n",
            "[1.1091584370249794, 0.7547619047619047]\n",
            "Epoch 1/70\n",
            "1680/1680 [==============================] - 7s 4ms/step - loss: 2.0429 - acc: 0.1333\n",
            "Epoch 2/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 2.0088 - acc: 0.1369\n",
            "Epoch 3/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9983 - acc: 0.1357\n",
            "Epoch 4/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9905 - acc: 0.1399\n",
            "Epoch 5/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9827 - acc: 0.1601\n",
            "Epoch 6/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9797 - acc: 0.1571\n",
            "Epoch 7/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9750 - acc: 0.1393\n",
            "Epoch 8/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9770 - acc: 0.1357\n",
            "Epoch 9/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9701 - acc: 0.1637\n",
            "Epoch 10/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9575 - acc: 0.1667\n",
            "Epoch 11/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9648 - acc: 0.1548\n",
            "Epoch 12/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9540 - acc: 0.1696\n",
            "Epoch 13/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9513 - acc: 0.1750\n",
            "Epoch 14/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9375 - acc: 0.1911\n",
            "Epoch 15/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9161 - acc: 0.2119\n",
            "Epoch 16/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9018 - acc: 0.2190\n",
            "Epoch 17/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9005 - acc: 0.2399\n",
            "Epoch 18/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8827 - acc: 0.2494\n",
            "Epoch 19/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8377 - acc: 0.2595\n",
            "Epoch 20/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8183 - acc: 0.2857\n",
            "Epoch 21/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.7859 - acc: 0.3060\n",
            "Epoch 22/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.7700 - acc: 0.3155\n",
            "Epoch 23/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.7079 - acc: 0.3417\n",
            "Epoch 24/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8710 - acc: 0.3167\n",
            "Epoch 25/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.6309 - acc: 0.3643\n",
            "Epoch 26/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.5514 - acc: 0.4190\n",
            "Epoch 27/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.5442 - acc: 0.4262\n",
            "Epoch 28/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.4761 - acc: 0.4363\n",
            "Epoch 29/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.4512 - acc: 0.4583\n",
            "Epoch 30/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.3625 - acc: 0.4923\n",
            "Epoch 31/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.4390 - acc: 0.4679\n",
            "Epoch 32/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.2458 - acc: 0.5345\n",
            "Epoch 33/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.1829 - acc: 0.5810\n",
            "Epoch 34/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.1760 - acc: 0.5720\n",
            "Epoch 35/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.1635 - acc: 0.5690\n",
            "Epoch 36/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0637 - acc: 0.6304\n",
            "Epoch 37/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0996 - acc: 0.5994\n",
            "Epoch 38/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.9408 - acc: 0.6595\n",
            "Epoch 39/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0424 - acc: 0.6286\n",
            "Epoch 40/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.9632 - acc: 0.6500\n",
            "Epoch 41/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.9196 - acc: 0.6673\n",
            "Epoch 42/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.8632 - acc: 0.6863\n",
            "Epoch 43/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.8198 - acc: 0.7107\n",
            "Epoch 44/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.8025 - acc: 0.7161\n",
            "Epoch 45/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7665 - acc: 0.7202\n",
            "Epoch 46/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7664 - acc: 0.7333\n",
            "Epoch 47/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7104 - acc: 0.7476\n",
            "Epoch 48/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6882 - acc: 0.7542\n",
            "Epoch 49/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5958 - acc: 0.7827\n",
            "Epoch 50/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5766 - acc: 0.7946\n",
            "Epoch 51/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5567 - acc: 0.8000\n",
            "Epoch 52/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6073 - acc: 0.7851\n",
            "Epoch 53/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5465 - acc: 0.8060\n",
            "Epoch 54/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4229 - acc: 0.8518\n",
            "Epoch 55/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5150 - acc: 0.8232\n",
            "Epoch 56/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4684 - acc: 0.8369\n",
            "Epoch 57/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4660 - acc: 0.8310\n",
            "Epoch 58/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4487 - acc: 0.8482\n",
            "Epoch 59/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3996 - acc: 0.8494\n",
            "Epoch 60/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4195 - acc: 0.8571\n",
            "Epoch 61/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4103 - acc: 0.8613\n",
            "Epoch 62/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3638 - acc: 0.8738\n",
            "Epoch 63/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3821 - acc: 0.8679\n",
            "Epoch 64/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3744 - acc: 0.8690\n",
            "Epoch 65/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4161 - acc: 0.8589\n",
            "Epoch 66/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3665 - acc: 0.8702\n",
            "Epoch 67/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3394 - acc: 0.8804\n",
            "Epoch 68/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3159 - acc: 0.8881\n",
            "Epoch 69/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3157 - acc: 0.8827\n",
            "Epoch 70/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2885 - acc: 0.8988\n",
            "[0.949284883907863, 0.7285714285714285]\n",
            "Epoch 1/70\n",
            "1680/1680 [==============================] - 7s 4ms/step - loss: 2.0463 - acc: 0.1399\n",
            "Epoch 2/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 2.0030 - acc: 0.1536\n",
            "Epoch 3/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9983 - acc: 0.1387\n",
            "Epoch 4/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9885 - acc: 0.1423\n",
            "Epoch 5/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9941 - acc: 0.1512\n",
            "Epoch 6/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9637 - acc: 0.1720\n",
            "Epoch 7/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9717 - acc: 0.1702\n",
            "Epoch 8/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9459 - acc: 0.1911\n",
            "Epoch 9/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9436 - acc: 0.1917\n",
            "Epoch 10/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9052 - acc: 0.2143\n",
            "Epoch 11/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.9485 - acc: 0.2077\n",
            "Epoch 12/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8443 - acc: 0.2595\n",
            "Epoch 13/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.8351 - acc: 0.2643\n",
            "Epoch 14/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.7559 - acc: 0.3071\n",
            "Epoch 15/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.7296 - acc: 0.3470\n",
            "Epoch 16/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.6665 - acc: 0.3560\n",
            "Epoch 17/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.5444 - acc: 0.4131\n",
            "Epoch 18/70\n",
            "1680/1680 [==============================] - 6s 4ms/step - loss: 1.6202 - acc: 0.4012\n",
            "Epoch 19/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.5446 - acc: 0.4238\n",
            "Epoch 20/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.4615 - acc: 0.4661\n",
            "Epoch 21/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.2769 - acc: 0.5268\n",
            "Epoch 22/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.3838 - acc: 0.5060\n",
            "Epoch 23/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.1932 - acc: 0.5708\n",
            "Epoch 24/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.1374 - acc: 0.5929\n",
            "Epoch 25/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.1779 - acc: 0.5738\n",
            "Epoch 26/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0219 - acc: 0.6393\n",
            "Epoch 27/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 1.0114 - acc: 0.6369\n",
            "Epoch 28/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.9147 - acc: 0.6696\n",
            "Epoch 29/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.8293 - acc: 0.7048\n",
            "Epoch 30/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.8939 - acc: 0.6839\n",
            "Epoch 31/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7550 - acc: 0.7375\n",
            "Epoch 32/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.8086 - acc: 0.7143\n",
            "Epoch 33/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7225 - acc: 0.7446\n",
            "Epoch 34/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.7410 - acc: 0.7530\n",
            "Epoch 35/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6610 - acc: 0.7726\n",
            "Epoch 36/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6283 - acc: 0.7732\n",
            "Epoch 37/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.6202 - acc: 0.7917\n",
            "Epoch 38/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5348 - acc: 0.8054\n",
            "Epoch 39/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5037 - acc: 0.8232\n",
            "Epoch 40/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5424 - acc: 0.8077\n",
            "Epoch 41/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4748 - acc: 0.8423\n",
            "Epoch 42/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4636 - acc: 0.8393\n",
            "Epoch 43/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.5370 - acc: 0.8167\n",
            "Epoch 44/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4145 - acc: 0.8589\n",
            "Epoch 45/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4188 - acc: 0.8554\n",
            "Epoch 46/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4109 - acc: 0.8619\n",
            "Epoch 47/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.4051 - acc: 0.8583\n",
            "Epoch 48/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3893 - acc: 0.8631\n",
            "Epoch 49/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3221 - acc: 0.8845\n",
            "Epoch 50/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3499 - acc: 0.8768\n",
            "Epoch 51/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3704 - acc: 0.8720\n",
            "Epoch 52/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3715 - acc: 0.8720\n",
            "Epoch 53/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2964 - acc: 0.8929\n",
            "Epoch 54/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3076 - acc: 0.8964\n",
            "Epoch 55/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3402 - acc: 0.8893\n",
            "Epoch 56/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3275 - acc: 0.8911\n",
            "Epoch 57/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3270 - acc: 0.8905\n",
            "Epoch 58/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.3002 - acc: 0.8875\n",
            "Epoch 59/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2840 - acc: 0.9048\n",
            "Epoch 60/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2427 - acc: 0.9185\n",
            "Epoch 61/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2107 - acc: 0.9274\n",
            "Epoch 62/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2656 - acc: 0.9054\n",
            "Epoch 63/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2110 - acc: 0.9310\n",
            "Epoch 64/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2657 - acc: 0.9042\n",
            "Epoch 65/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2076 - acc: 0.9280\n",
            "Epoch 66/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.1603 - acc: 0.9488\n",
            "Epoch 67/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2458 - acc: 0.9190\n",
            "Epoch 68/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2153 - acc: 0.9262\n",
            "Epoch 69/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.2601 - acc: 0.9155\n",
            "Epoch 70/70\n",
            "1680/1680 [==============================] - 6s 3ms/step - loss: 0.1755 - acc: 0.9327\n",
            "[0.8581925778161912, 0.780952380952381]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}